# AI Engineer Toolkit

## Skill Map

### Must-Learn AI Engineer Toolkit

| Area | Why |
| --- | --- |
| Transformers (practical) | Foundation of GenAI |
| LLM APIs (OpenAI, Anthropic, open models) | Most jobs use them |
| Prompt engineering | Real-world performance driver |
| RAG architectures | Most enterprise GenAI |
| Fine-tuning (LoRA) | Customization |
| Inference optimization | Cost & latency |

### Optional

- Training from scratch
- Deep theoretical proofs
- GAN math
- Reinforcement learning theory (except RLHF intuition)

## 90-Day Plan

### Month 1: Core Foundations (Hands-On)

> Goal: Understand how LLMs work + use them fluently

#### Week 1–2: Transformers & LLM Basics

- Learn:
    - Self-attention (QKV intuition)
    - Tokenization (BPE)
    - Why scaling works
    - Decoder-only models (GPT-style)
- Resources:
    - [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
    - Karpathy’s “Let’s build GPT” (skim, don’t over-optimize)
    - HuggingFace Transformers docs

- Deliverable:
    - Load a LLaMA / Mistral model
    - Generate text locally

#### Week 3–4: Prompting + APIs

- Learn:
    - Prompt templates
    - Few-shot vs zero-shot
    - Chain-of-thought
    - Tool calling
    - Structured outputs (JSON)


- Deliverable:
    - Build a prompt-driven text classifier
    - Compare prompting vs fine-tuning
