- [DL Engineer Toolkit](#dl-engineer-toolkit)
  - [é˜¶æ®µ 0ï¼ˆå¯å¹¶è¡Œï¼‰ï¼šå·¥å…·ä¸ç¼–ç¨‹åŸºç¡€ï¼ˆè´¯ç©¿å…¨ç¨‹ï¼‰](#é˜¶æ®µ-0å¯å¹¶è¡Œå·¥å…·ä¸ç¼–ç¨‹åŸºç¡€è´¯ç©¿å…¨ç¨‹)
  - [é˜¶æ®µ 1ï¼šæ•°å­¦ä¸æœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆ0â€“3 ä¸ªæœˆï¼‰](#é˜¶æ®µ-1æ•°å­¦ä¸æœºå™¨å­¦ä¹ åŸºç¡€03-ä¸ªæœˆ)
    - [æ•°å­¦ï¼ˆéå¸¸é‡è¦ï¼‰](#æ•°å­¦éå¸¸é‡è¦)
    - [ä¼ ç»Ÿæœºå™¨å­¦ä¹ ](#ä¼ ç»Ÿæœºå™¨å­¦ä¹ )
  - [é˜¶æ®µ 2ï¼šæ·±åº¦å­¦ä¹ æ ¸å¿ƒï¼ˆ3â€“6 ä¸ªæœˆï¼‰](#é˜¶æ®µ-2æ·±åº¦å­¦ä¹ æ ¸å¿ƒ36-ä¸ªæœˆ)
    - [ç¥ç»ç½‘ç»œåŸºç¡€](#ç¥ç»ç½‘ç»œåŸºç¡€)
    - [CNN / RNN / Attention](#cnn--rnn--attention)
  - [é˜¶æ®µ 3ï¼šè®­ç»ƒæŠ€å·§ \& Debug èƒ½åŠ›ï¼ˆ6â€“9 ä¸ªæœˆï¼‰](#é˜¶æ®µ-3è®­ç»ƒæŠ€å·§--debug-èƒ½åŠ›69-ä¸ªæœˆ)
  - [é˜¶æ®µ 4ï¼šé«˜çº§æ¨¡å‹ä¸æ–¹å‘æ·±åŒ–ï¼ˆ9â€“15 ä¸ªæœˆï¼‰](#é˜¶æ®µ-4é«˜çº§æ¨¡å‹ä¸æ–¹å‘æ·±åŒ–915-ä¸ªæœˆ)
    - [æ–¹å‘ Aï¼šåºåˆ—å»ºæ¨¡ \& æ¨èç³»ç»Ÿï¼ˆä½ éå¸¸é€‚åˆï¼‰](#æ–¹å‘-aåºåˆ—å»ºæ¨¡--æ¨èç³»ç»Ÿä½ éå¸¸é€‚åˆ)
    - [æ–¹å‘ Bï¼šç”Ÿæˆæ¨¡å‹](#æ–¹å‘-bç”Ÿæˆæ¨¡å‹)
    - [æ–¹å‘ Cï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆDeepMind æ ¸å¿ƒï¼‰](#æ–¹å‘-cå¼ºåŒ–å­¦ä¹ deepmind-æ ¸å¿ƒ)
  - [é˜¶æ®µ 5ï¼šç§‘ç ”èƒ½åŠ›ï¼ˆ12â€“18 ä¸ªæœˆï¼‰](#é˜¶æ®µ-5ç§‘ç ”èƒ½åŠ›1218-ä¸ªæœˆ)
- [DeepMindé£æ ¼ç ”ç©¶é¢˜](#deepmindé£æ ¼ç ”ç©¶é¢˜)
  - [é¢˜ç›® 1ï¼ˆæ ¸å¿ƒæ¨èï¼‰ï¼šå¤šåºåˆ—ç”¨æˆ·è¡Œä¸ºçš„å› æœè¡¨å¾å­¦ä¹ ](#é¢˜ç›®-1æ ¸å¿ƒæ¨èå¤šåºåˆ—ç”¨æˆ·è¡Œä¸ºçš„å› æœè¡¨å¾å­¦ä¹ )
    - [èƒŒæ™¯ï¼ˆDeepMind é£æ ¼ï¼‰](#èƒŒæ™¯deepmind-é£æ ¼)
    - [ä½ éœ€è¦å›ç­”çš„é—®é¢˜](#ä½ éœ€è¦å›ç­”çš„é—®é¢˜)
      - [Q1ï¼šæ¨¡å‹è®¾è®¡](#q1æ¨¡å‹è®¾è®¡)
      - [Q2ï¼šå› æœæŒ‘æˆ˜](#q2å› æœæŒ‘æˆ˜)
      - [Q3ï¼šå®éªŒè®¾è®¡](#q3å®éªŒè®¾è®¡)
    - [åŠ åˆ†é¡¹ï¼ˆéå¸¸ DeepMindï¼‰](#åŠ åˆ†é¡¹éå¸¸-deepmind)
    - [è€ƒå¯Ÿç‚¹](#è€ƒå¯Ÿç‚¹)
  - [é¢˜ç›® 2ï¼ˆå¼ºåŒ–å­¦ä¹ æ–¹å‘ï¼‰ï¼šé•¿æ—¶å»¶å¥–åŠ±ä¸‹çš„ç”¨æˆ·è½¬åŒ–å»ºæ¨¡](#é¢˜ç›®-2å¼ºåŒ–å­¦ä¹ æ–¹å‘é•¿æ—¶å»¶å¥–åŠ±ä¸‹çš„ç”¨æˆ·è½¬åŒ–å»ºæ¨¡)
    - [èƒŒæ™¯](#èƒŒæ™¯)
    - [ä½ éœ€è¦å›ç­”çš„é—®é¢˜](#ä½ éœ€è¦å›ç­”çš„é—®é¢˜-1)
      - [Q1ï¼šçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å¦‚ä½•å®šä¹‰ï¼Ÿ](#q1çŠ¶æ€åŠ¨ä½œå¥–åŠ±å¦‚ä½•å®šä¹‰)
      - [Q2ï¼šå¦‚ä½•å¤„ç†ç¨€ç–å¥–åŠ±ï¼Ÿ](#q2å¦‚ä½•å¤„ç†ç¨€ç–å¥–åŠ±)
      - [Q3ï¼šä¸ºä»€ä¹ˆ Policy Gradient æ¯” Q-learning æ›´é€‚åˆï¼Ÿ](#q3ä¸ºä»€ä¹ˆ-policy-gradient-æ¯”-q-learning-æ›´é€‚åˆ)
      - [Q4ï¼šæ˜¯å¦å¯ä»¥ç”¨ model-based RLï¼Ÿ](#q4æ˜¯å¦å¯ä»¥ç”¨-model-based-rl)
      - [Q5ï¼šå¦‚ä½•ç¨³å®šè®­ç»ƒï¼Ÿ](#q5å¦‚ä½•ç¨³å®šè®­ç»ƒ)
      - [Q6ï¼šå¦‚ä½•è¯„ä»· learned policyï¼Ÿ](#q6å¦‚ä½•è¯„ä»·-learned-policy)
    - [åŠ åˆ†é¡¹](#åŠ åˆ†é¡¹)
    - [è€ƒå¯Ÿç‚¹](#è€ƒå¯Ÿç‚¹-1)
  - [é¢˜ç›® 3ï¼ˆåŸºç¡€ç ”ç©¶å–å‘ï¼‰ï¼šAttention æ˜¯å¦çœŸçš„éœ€è¦ Softmaxï¼Ÿ](#é¢˜ç›®-3åŸºç¡€ç ”ç©¶å–å‘attention-æ˜¯å¦çœŸçš„éœ€è¦-softmax)
    - [èƒŒæ™¯](#èƒŒæ™¯-1)
    - [ä»»åŠ¡](#ä»»åŠ¡)
    - [ä½ éœ€è¦åšçš„](#ä½ éœ€è¦åšçš„)
    - [åŠ åˆ†é¡¹](#åŠ åˆ†é¡¹-1)
    - [è€ƒå¯Ÿç‚¹](#è€ƒå¯Ÿç‚¹-2)

# DL Engineer Toolkit

> ğŸ¯ æ€»ç›®æ ‡ï¼ˆ12â€“18 ä¸ªæœˆï¼‰

åˆ°è®¡åˆ’ç»“æŸæ—¶ï¼Œä½ åº”å½“èƒ½å¤Ÿï¼š
- ç‹¬ç«‹è®¾è®¡å’Œå®ç°å¤æ‚ ML / DL æ¨¡å‹
- è¯»æ‡‚å¹¶å¤ç° NeurIPS / ICML / ICLR è®ºæ–‡
- ç³»ç»Ÿæ€§ debug è®­ç»ƒé—®é¢˜ï¼ˆNaNã€æ¢¯åº¦çˆ†ç‚¸ã€æ”¶æ•›å¤±è´¥ï¼‰
- å…·å¤‡ç”³è¯· DeepMind / OpenAI / FAIR / é¡¶çº§ PhD æˆ–ç ”ç©¶å‹å²—ä½çš„èƒ½åŠ›

## é˜¶æ®µ 0ï¼ˆå¯å¹¶è¡Œï¼‰ï¼šå·¥å…·ä¸ç¼–ç¨‹åŸºç¡€ï¼ˆè´¯ç©¿å…¨ç¨‹ï¼‰

å¿…é¡»æŒæ¡

- Pythonï¼ˆNumPy / SciPy / Pandasï¼‰
- é¢å‘å¯¹è±¡è®¾è®¡ï¼ˆå°¤å…¶æ˜¯ DL module è®¾è®¡ï¼‰
- Linux / shell / git
- GPU åŸºç¡€ï¼ˆæ˜¾å­˜ã€batch sizeã€OOMï¼‰

ğŸ¯ äº§å‡ºæ ‡å‡†
- èƒ½å†™ç»“æ„æ¸…æ™°ã€å¯å¤ç”¨çš„ ML æ¨¡å—
- èƒ½ debug shape / dtype / device é—®é¢˜

## é˜¶æ®µ 1ï¼šæ•°å­¦ä¸æœºå™¨å­¦ä¹ åŸºç¡€ï¼ˆ0â€“3 ä¸ªæœˆï¼‰

### æ•°å­¦ï¼ˆéå¸¸é‡è¦ï¼‰

ä½ éœ€è¦ ç†è§£ï¼Œè€Œä¸æ˜¯æ­»è®°ã€‚

å¿…é¡»æŒæ¡

- çº¿æ€§ä»£æ•°
  - å‘é‡ç©ºé—´ã€çŸ©é˜µä¹˜æ³•ã€ç‰¹å¾å€¼/ç‰¹å¾å‘é‡
  - æŠ•å½±ã€æ­£äº¤ã€SVD
- æ¦‚ç‡ä¸ç»Ÿè®¡
  - éšæœºå˜é‡ã€æœŸæœ›ã€æ–¹å·®
  - MLE / MAP
  - KL divergenceã€Entropy
- ä¼˜åŒ–
  - æ¢¯åº¦ä¸‹é™
  - å‡¸/éå‡¸ä¼˜åŒ–
  - å­¦ä¹ ç‡å½±å“

æ¨èï¼š

- Mathematics for Machine Learning
- MIT OCW Linear Algebra

ğŸ¯ äº§å‡ºæ ‡å‡†

- èƒ½ä»å…¬å¼æ¨å¯¼å‡º loss å’Œ gradient
- èƒ½è§£é‡Šä¸ºä»€ä¹ˆæŸä¸ªä¼˜åŒ–ä¸æ”¶æ•›

### ä¼ ç»Ÿæœºå™¨å­¦ä¹ 

å¿…é¡»æŒæ¡

- [Linear](ML-Code/src/regression.py) / [Logistic](ML-Code/src/classification.py) Regression
- SVMï¼ˆmarginã€kernelï¼‰
- Decision Tree / Random Forest / GBDT
- KNN
- K-means
- **Biasâ€“Variance tradeoff**
  - Metrics: train set error vs. validation set error
    - low training error, large validation error (overfit): high variance
    - large training error, large validation error (underfit): high bias
    - large training error, larger validation error (not enough data): high bias, high variance
    - low training error, low validation error (just right): low bias, low variance
  - Recipe
    - High bias: bigger network, train longer
    - High variance: more data, regularization
- **Regularization**
  - L1 / L2
    - L1: sparsity (meaning a lot of zeros)
    - L2: smoothness (also called weight decay)
  - Dropout
  - Early stopping
- **Feature engineering**
  - 

ğŸ“˜ æ¨èï¼š

- [Andrew Ng ML](https://www.coursera.org/specializations/machine-learning-introduction)
  - [Youtube video playlist](https://www.youtube.com/watch?v=vStJoetOxJg&list=PLkDaE6sCZn6FNC6YRfRQc_FbeQrF8BwGI)
- [Elements of Statistical Learningï¼ˆHastieï¼‰](https://hastie.su.domains/ElemStatLearn/)

ğŸ¯ äº§å‡ºæ ‡å‡†

- èƒ½è§£é‡Šä¸ºä»€ä¹ˆæŸæ¨¡å‹ overfit
- èƒ½æ‰‹å†™ loss + gradient

## é˜¶æ®µ 2ï¼šæ·±åº¦å­¦ä¹ æ ¸å¿ƒï¼ˆ3â€“6 ä¸ªæœˆï¼‰

### ç¥ç»ç½‘ç»œåŸºç¡€

å¿…é¡»æŒæ¡

- Backpropagation
- Activationï¼ˆReLU / GELU / SiLUï¼‰
- Initializationï¼ˆXavier / Heï¼‰
- Normalizationï¼ˆBatchNorm / LayerNormï¼‰

ğŸ¯ äº§å‡ºæ ‡å‡†

- èƒ½ä»å¤´å†™ä¸€ä¸ª MLPï¼ˆä¸ç”¨ Keras Sequentialï¼‰
- èƒ½è§£é‡Šæ¢¯åº¦æ¶ˆå¤± / çˆ†ç‚¸

### CNN / RNN / Attention

- CNN
  - Convolutionã€Poolingã€Receptive Field

- RNN
  - RNN / LSTM / GRU
  - Sequence masking
  - Truncated BPTT

- Attention & Transformerï¼ˆé‡ç‚¹ï¼‰
  - Scaled dot-product attention
  - Multi-head attention
  - Positional encoding
  - Encoder / Decoder

æ¨èï¼š

- [CS231n](https://cs231n.stanford.edu/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Jay Alammar åšå®¢](https://jalammar.github.io/)

ğŸ¯ äº§å‡ºæ ‡å‡†

- èƒ½æ‰‹å†™ Transformer Encoder
- èƒ½æ­£ç¡®å¤„ç† mask
- èƒ½ç†è§£ä¸ºä»€ä¹ˆ attention æ¯” RNN å¥½

## é˜¶æ®µ 3ï¼šè®­ç»ƒæŠ€å·§ & Debug èƒ½åŠ›ï¼ˆ6â€“9 ä¸ªæœˆï¼‰

è¿™æ˜¯ä½ ä¹‹å‰é—®çš„ NaNã€batch sizeã€maskã€sequence çš„æ ¹æœ¬èƒ½åŠ›æ¥æºã€‚

å¿…é¡»æŒæ¡

- Loss scale / mixed precision
- Gradient clipping
- Learning rate schedule
- Batch size vs convergence
- NaN / Inf debug
- æ•°å€¼ç¨³å®šæ€§ï¼ˆlog-sum-expï¼‰

ğŸ¯ äº§å‡ºæ ‡å‡†

- èƒ½å¿«é€Ÿå®šä½ NaN æ¥æº
- èƒ½è§£é‡Š batch size æ”¹å˜ä¸ºä»€ä¹ˆå½±å“ loss

## é˜¶æ®µ 4ï¼šé«˜çº§æ¨¡å‹ä¸æ–¹å‘æ·±åŒ–ï¼ˆ9â€“15 ä¸ªæœˆï¼‰

ä½ å¯ä»¥é€‰ 1â€“2 ä¸ªæ–¹å‘æ·±å…¥ï¼ˆå»ºè®®é€‰ä¸ä½ ç°åœ¨å·¥ä½œç›¸å…³çš„ï¼‰ï¼š

### æ–¹å‘ Aï¼šåºåˆ—å»ºæ¨¡ & æ¨èç³»ç»Ÿï¼ˆä½ éå¸¸é€‚åˆï¼‰

- DIN / DIEN
- Transformer4Rec
- Self-attention vs Cross-attention
- Pooling / attention pooling
- Long sequence modeling

ğŸ¯ é¡¹ç›®ç¤ºä¾‹ï¼š

- ç”¨æˆ·äº‹ä»¶åºåˆ— â†’ è½¬åŒ–é¢„æµ‹æ¨¡å‹
- å¤šåºåˆ— + cross attention

### æ–¹å‘ Bï¼šç”Ÿæˆæ¨¡å‹

- AutoEncoder / VAE
- Diffusion models
- Representation learning

### æ–¹å‘ Cï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆDeepMind æ ¸å¿ƒï¼‰

- Q-learning
- Policy Gradient
- PPO / SAC
- Model-based RL

ğŸ“˜ æ¨èï¼š

- Sutton & Barto
- Spinning Up in Deep RL

## é˜¶æ®µ 5ï¼šç§‘ç ”èƒ½åŠ›ï¼ˆ12â€“18 ä¸ªæœˆï¼‰

å¿…é¡»æŒæ¡

- è¯»è®ºæ–‡ï¼ˆé—®é¢˜ â†’ æ–¹æ³• â†’ å®éªŒï¼‰
- åšæ¶ˆèå®éªŒ
- å†™ research-style ä»£ç 
- ç»“æœå¤ç°

ğŸ¯ äº§å‡º

- GitHub repoï¼ˆå¤ç° + æ”¹è¿›ï¼‰
- arXiv / workshop è®ºæ–‡ï¼ˆå“ªæ€•æ˜¯å°æ”¹è¿›ï¼‰

# DeepMindé£æ ¼ç ”ç©¶é¢˜

## é¢˜ç›® 1ï¼ˆæ ¸å¿ƒæ¨èï¼‰ï¼šå¤šåºåˆ—ç”¨æˆ·è¡Œä¸ºçš„å› æœè¡¨å¾å­¦ä¹ 

### èƒŒæ™¯ï¼ˆDeepMind é£æ ¼ï¼‰

ä½ æœ‰ç”¨æˆ·çš„å¤šç§è¡Œä¸ºåºåˆ—ï¼ˆapp installã€purchaseã€viewã€clickï¼‰ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ æ˜¯å¦å‘ç”Ÿè½¬åŒ–ã€‚

ç°å®é—®é¢˜ï¼š
- è¡Œä¸ºåºåˆ—é•¿åº¦ä¸ä¸€è‡´
- ä¸åŒè¡Œä¸ºå¯¹è½¬åŒ–çš„å› æœå½±å“ä¸åŒ
- ç®€å• attention ä¼šâ€œè®°ä½ç›¸å…³æ€§â€ï¼Œè€Œä¸æ˜¯å› æœæ€§

ğŸ¯ ä»»åŠ¡
- è®¾è®¡ä¸€ä¸ªæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿï¼š
    - åˆ©ç”¨å¤šç§äº‹ä»¶åºåˆ—
    - å­¦åˆ° å¯¹è½¬åŒ–çœŸæ­£æœ‰å› æœè´¡çŒ®çš„è¡¨ç¤º
    - åœ¨å­˜åœ¨â€œè™šå‡ç›¸å…³â€çš„æƒ…å†µä¸‹ä»ä¿æŒç¨³å®šé¢„æµ‹

### ä½ éœ€è¦å›ç­”çš„é—®é¢˜

ï¼ˆè¿™æ­£æ˜¯ DeepMind é¢è¯•ä¼šé—®çš„ï¼‰

#### Q1ï¼šæ¨¡å‹è®¾è®¡

ä½ ä¼šå¦‚ä½•å»ºæ¨¡å¤šåºåˆ—ï¼Ÿ

self-attention vs cross-attention å¦‚ä½•é€‰æ‹©ï¼Ÿ

æ˜¯å¦éœ€è¦å…±äº« embeddingï¼Ÿ

#### Q2ï¼šå› æœæŒ‘æˆ˜

å“ªäº›è¡Œä¸ºåªæ˜¯â€œç›¸å…³ä½†éå› æœâ€ï¼Ÿ

å¦‚ä½•è®¾è®¡æ¨¡å‹æˆ–è®­ç»ƒç›®æ ‡å‡å°‘ spurious correlationï¼Ÿ

#### Q3ï¼šå®éªŒè®¾è®¡

ä½ å¦‚ä½•éªŒè¯æ¨¡å‹å­¦åˆ°äº†å› æœï¼Œè€Œä¸æ˜¯ç›¸å…³ï¼Ÿ

å¦‚æœ offline metrics æå‡ä½† online ä¸‹é™ï¼Œä½ å¦‚ä½•è§£é‡Šï¼Ÿ


### åŠ åˆ†é¡¹ï¼ˆéå¸¸ DeepMindï¼‰

- å¼•å…¥ counterfactual masking
- ä½¿ç”¨ temporal interventionï¼ˆæ‰“ä¹±æ—¶é—´é¡ºåºï¼‰
- å¼•å…¥è¾…åŠ© lossï¼ˆå¦‚ predicting masked eventï¼‰

### è€ƒå¯Ÿç‚¹

- è¡¨ç¤ºå­¦ä¹ 
- å› æœæ¨ç†ç›´è§‰
- attention æœºåˆ¶ç†è§£
- å®éªŒè®¾è®¡èƒ½åŠ›

## é¢˜ç›® 2ï¼ˆå¼ºåŒ–å­¦ä¹ æ–¹å‘ï¼‰ï¼šé•¿æ—¶å»¶å¥–åŠ±ä¸‹çš„ç”¨æˆ·è½¬åŒ–å»ºæ¨¡

### èƒŒæ™¯

è½¬åŒ–å¯èƒ½å‘ç”Ÿåœ¨ç”¨æˆ·è¡Œä¸ºåºåˆ—ç»“æŸ å‡ å°æ—¶ç”šè‡³å‡ å¤©ä¹‹åã€‚

ä¼ ç»Ÿ supervised learningï¼š
- åªçœ‹æ˜¯å¦è½¬åŒ–
- å¿½ç•¥è¡Œä¸ºä¸å¥–åŠ±çš„æ—¶é—´ç»“æ„

ğŸ¯ ä»»åŠ¡
- å°†è½¬åŒ–é¢„æµ‹é—®é¢˜é‡æ–°å»ºæ¨¡ä¸º å¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚

### ä½ éœ€è¦å›ç­”çš„é—®é¢˜

#### Q1ï¼šçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å¦‚ä½•å®šä¹‰ï¼Ÿ

#### Q2ï¼šå¦‚ä½•å¤„ç†ç¨€ç–å¥–åŠ±ï¼Ÿ

#### Q3ï¼šä¸ºä»€ä¹ˆ Policy Gradient æ¯” Q-learning æ›´é€‚åˆï¼Ÿ

#### Q4ï¼šæ˜¯å¦å¯ä»¥ç”¨ model-based RLï¼Ÿ

#### Q5ï¼šå¦‚ä½•ç¨³å®šè®­ç»ƒï¼Ÿ

#### Q6ï¼šå¦‚ä½•è¯„ä»· learned policyï¼Ÿ


### åŠ åˆ†é¡¹

- Credit assignmentï¼ˆTD(Î»)ï¼‰
- Use hindsight replay
- Offline RL

### è€ƒå¯Ÿç‚¹

- RL å»ºæ¨¡èƒ½åŠ›
- æŠ½è±¡ç°å®é—®é¢˜èƒ½åŠ›
- ç†è®ºä¸å·¥ç¨‹ç»“åˆ

## é¢˜ç›® 3ï¼ˆåŸºç¡€ç ”ç©¶å–å‘ï¼‰ï¼šAttention æ˜¯å¦çœŸçš„éœ€è¦ Softmaxï¼Ÿ

### èƒŒæ™¯

Transformer çš„æ ¸å¿ƒæ˜¯ï¼š

Attention(Q,K,V) = softmax(QK^T)V

ä½†ï¼š

- softmax å¸¦æ¥æ•°å€¼ä¸ç¨³å®š
- attention åˆ†å¸ƒè¿‡äºå¹³æ»‘

### ä»»åŠ¡

ç ”ç©¶å¹¶å›ç­”ï¼š

- æ˜¯å¦å­˜åœ¨ä¸ä½¿ç”¨ softmax çš„ attention æœºåˆ¶ï¼Ÿ
- å¦‚æœå­˜åœ¨ï¼Œå®ƒæœ‰ä»€ä¹ˆä¼˜ç‚¹ï¼Ÿ

### ä½ éœ€è¦åšçš„

- æå‡ºä¸€ç§æ›¿ä»£æœºåˆ¶ï¼ˆå¦‚ linear attentionï¼‰
- åˆ†æç†è®ºæ€§è´¨ï¼ˆå½’ä¸€åŒ–ã€ç¨³å®šæ€§ï¼‰
- å®éªŒéªŒè¯æ€§èƒ½

### åŠ åˆ†é¡¹

- åˆ†æå¤æ‚åº¦ O(NÂ²) â†’ O(N)
- é•¿åºåˆ—å»ºæ¨¡å®éªŒ
- å¯¹æ¯”æ”¶æ•›é€Ÿåº¦

### è€ƒå¯Ÿç‚¹

- æ•°å­¦ç›´è§‰
- ç®—æ³•åˆ›æ–°èƒ½åŠ›
- ç†è®º + å®éªŒç»“åˆ