- [AI Engineer Toolkit](#ai-engineer-toolkit)
  - [Skill Map](#skill-map)
    - [Must-Learn AI Engineer Toolkit](#must-learn-ai-engineer-toolkit)
    - [Optional](#optional)
  - [90-Day Study Plan](#90-day-study-plan)
    - [Month 1: Core Foundations (Hands-On)](#month-1-core-foundations-hands-on)
      - [Week 1â€“2: Transformers \& LLM Basics](#week-12-transformers--llm-basics)
      - [Week 3â€“4: Prompting + APIs](#week-34-prompting--apis)
    - [Month 2: Real Systems (This Is Where You Shine)](#month-2-real-systems-this-is-where-you-shine)
      - [Week 5â€“6: RAG (Critical Skill)](#week-56-rag-critical-skill)
      - [Week 7â€“8: Fine-Tuning \& PEFT](#week-78-fine-tuning--peft)
    - [Month 3: Production + Differentiation](#month-3-production--differentiation)
      - [Week 9â€“10: Evaluation \& Safety](#week-910-evaluation--safety)
      - [Week 11â€“12: Deployment \& Optimization](#week-1112-deployment--optimization)
    - [Projects That Get You Hired (Very Important)](#projects-that-get-you-hired-very-important)
      - [Project 1: RAG for Real Use Case](#project-1-rag-for-real-use-case)
      - [Project 2: Multi-Task GenAI System](#project-2-multi-task-genai-system)
      - [Project 3: LLM Evaluation Framework](#project-3-llm-evaluation-framework)
  - [æ‰‹æ“LLM](#æ‰‹æ“llm)
    - [é˜¶æ®µ 1ï¼šä»é›¶å®ç° Transformerï¼ˆæœ€é‡è¦ï¼‰](#é˜¶æ®µ-1ä»é›¶å®ç°-transformeræœ€é‡è¦)
    - [é˜¶æ®µ 2ï¼šAttention å˜ä½“ \& æ•°å€¼ç¨³å®šæ€§](#é˜¶æ®µ-2attention-å˜ä½“--æ•°å€¼ç¨³å®šæ€§)
    - [é˜¶æ®µ 3ï¼šTokenizer \& Embeddingï¼ˆå¾ˆå¤šäººå¿½ç•¥ï¼‰](#é˜¶æ®µ-3tokenizer--embeddingå¾ˆå¤šäººå¿½ç•¥)
    - [é˜¶æ®µ 4ï¼šè®­ç»ƒæŠ€å·§ï¼ˆå·¥ä¸šçº§ï¼‰](#é˜¶æ®µ-4è®­ç»ƒæŠ€å·§å·¥ä¸šçº§)
    - [é˜¶æ®µ 5ï¼šæ¨ç† \& è§£ç ](#é˜¶æ®µ-5æ¨ç†--è§£ç )
    - [é˜¶æ®µ 6ï¼šDeepMind é£æ ¼è¿›é˜¶é¡¹ç›®](#é˜¶æ®µ-6deepmind-é£æ ¼è¿›é˜¶é¡¹ç›®)
      - [é¡¹ç›® Aï¼šé•¿åºåˆ—è¯­è¨€å»ºæ¨¡](#é¡¹ç›®-aé•¿åºåˆ—è¯­è¨€å»ºæ¨¡)
      - [é¡¹ç›® Bï¼šLLM + åºåˆ—æ¨è](#é¡¹ç›®-bllm--åºåˆ—æ¨è)
    - [DeepMind é¢è¯•çº§è¿½é—®ï¼ˆä½ å¿…é¡»èƒ½ç­”ï¼‰](#deepmind-é¢è¯•çº§è¿½é—®ä½ å¿…é¡»èƒ½ç­”)
    - [å­¦ä¹ é¡ºåºï¼ˆæœ€æ¨èï¼‰](#å­¦ä¹ é¡ºåºæœ€æ¨è)
  - [References](#references)


# AI Engineer Toolkit

## Skill Map

### Must-Learn AI Engineer Toolkit

| Area | Why |
| --- | --- |
| Transformers (practical) | Foundation of GenAI |
| LLM APIs (OpenAI, Anthropic, open models) | Most jobs use them |
| Prompt engineering | Real-world performance driver |
| RAG architectures | Most enterprise GenAI |
| Fine-tuning (LoRA) | Customization |
| Inference optimization | Cost & latency |

### Optional

- Training from scratch
- Deep theoretical proofs
- GAN math
- Reinforcement learning theory (except RLHF intuition)

## 90-Day Study Plan

| Month | Topics | Week breakdowns |
| --- | --- | --- |
| 1 | [Core Foundations (Hands-On)](#month-1-core-foundations-hands-on) | [Week 1â€“2: Transformers & LLM Basics](#week-12-transformers--llm-basics) <br> [Week 3â€“4: Prompting + APIs](#week-34-prompting--apis) |
| 2 | [Real Systems (This Is Where You Shine)](#month-2-real-systems-this-is-where-you-shine) | [Week 5â€“6: RAG (Critical Skill)](#week-56-rag-critical-skill) <br> [Week 7â€“8: Fine-Tuning & PEFT](#week-78-fine-tuning--peft) |
| 3 | [Production + Differentiation](#month-3-production-differentiation) | [Week 9â€“10: Evaluation & Safety](#week-910-evaluation--safety) <br> [Week 11â€“12: Deployment & Optimization](#week-1112-deployment--optimization) |


### Month 1: Core Foundations (Hands-On)

> Goal: Understand how LLMs work + use them fluently

#### Week 1â€“2: Transformers & LLM Basics

- Learn:
  - [Self-attention (QKV intuition)](./self_attention.md)
  - Tokenization (BPE)
  - Why scaling works
  - Decoder-only models (GPT-style)
- Resources:
  - [Andrej Karpathy: Nerual Networks - Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
  - [Attention is All You Need](https://arxiv.org/pdf/1706.03762)
  - [Understanding and Coding the Self-Attention Mechanism of LLM From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
  - [Understanding Large Language Models -- A Transformative Reading List](https://sebastianraschka.com/blog/2023/llm-reading-list.html)
  - [Visualizing Neural Machine Translation: Mechanics of Seq2Seq Models with Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention)
  - [Understanding Attention Mechanism](https://medium.com/@shashank7.iitd/understanding-attention-mechanism-35ff53fc328e)
  - [Attn Illustrated: Attention](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)
  - [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
  - [Self-Attention Mechanisms in Natural Language Processing](https://medium.com/@Alibaba_Cloud/self-attention-mechanisms-in-natural-language-processing-9f28315ff905)
  - [Illustrated Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)
  - [Letâ€™s build GPT](https://www.youtube.com/watch?v=Qz6p0zZ6zZM)
  - [HuggingFace Transformers docs](https://huggingface.co/docs/transformers/en/index)

- Deliverable:
  - Load a LLaMA / Mistral model
  - Generate text locally

#### Week 3â€“4: Prompting + APIs

- Learn:
  - Prompt templates
  - Few-shot vs zero-shot
  - Chain-of-thought
  - Tool calling
  - Structured outputs (JSON)


- Deliverable:
  - Build a prompt-driven text classifier
  - Compare prompting vs fine-tuning


### Month 2: Real Systems (This Is Where You Shine)

> Goal: Build production-like GenAI systems

#### Week 5â€“6: RAG (Critical Skill)

- Learn:
    - Embeddings
    - Vector databases (FAISS, Pinecone)
    - Chunking strategies
    - Retrieval evaluation


- Deliverable:
    - Build a document Q&A system
    - Measure recall@k
    - Handle hallucinations

#### Week 7â€“8: Fine-Tuning & PEFT

- Learn:
    - LoRA
    - Instruction tuning
    - When fine-tuning beats prompting
    - Data quality > data quantity

- Deliverable:
    - Fine-tune a small LLM on a domain task
    - Compare with prompting baseline

### Month 3: Production + Differentiation

> Goal: Be job-ready

#### Week 9â€“10: Evaluation & Safety

- Learn:
    - LLM evaluation is hard
    - Automatic metrics (BLEU â‰  enough)
    - Human eval frameworks
    - Guardrails

- Deliverable:
    - Build an evaluation harness
    - Track cost / latency / quality trade-offs

#### Week 11â€“12: Deployment & Optimization

- Learn:
    - Quantization (4-bit / 8-bit)
    - Batch inference
    - Caching
    - Streaming generation

- Deliverable:
    - Deploy a GenAI service
    - Cost breakdown analysis

### Projects That Get You Hired (Very Important)

These are strong signals to hiring managers:
#### Project 1: RAG for Real Use Case

Example:
â€œAI assistant for app monetization / ads optimization docsâ€

Show:
- Chunking strategy
- Retrieval metrics
- Failure analysis



#### Project 2: Multi-Task GenAI System

Tie to your background:
- Generate ad copy
- Predict conversion likelihood
- Explain why copy works


This shows hybrid GenAI + ranking expertise.

#### Project 3: LLM Evaluation Framework

Very few candidates do this well.


## æ‰‹æ“LLM

ğŸ§  æ€»ä½“å­¦ä¹ è·¯å¾„ï¼ˆä» 0 â†’ LLM å†…æ ¸ï¼‰

ä½ ä¸éœ€è¦ä¸€å¼€å§‹å°±è®­ç»ƒ 7B æ¨¡å‹ï¼Œè€Œæ˜¯ï¼š

> å°æ¨¡å‹ + æ­£ç¡®ç»“æ„ + å¯æ§å®éªŒ â†’ æ‰©å±•åˆ° LLM

### é˜¶æ®µ 1ï¼šä»é›¶å®ç° Transformerï¼ˆæœ€é‡è¦ï¼‰

ğŸ¯ ç›®æ ‡

- å®Œå…¨ç†è§£å¹¶æ‰‹å†™ï¼š
    - Token embedding
    - Positional encoding
    - Self-attentionï¼ˆQKVï¼‰
    - Multi-head attention
    - FFN
    - LayerNorm
    - Maskï¼ˆcausal / paddingï¼‰
    - Autoregressive decoding

âœ… å¼ºçƒˆæ¨èçš„ repoï¼ˆæŒ‰æ•™å­¦ä»·å€¼ï¼‰

- 1ï¸âƒ£ [nanoGPT â­â­â­â­â­ï¼ˆå¿…åšï¼‰](https://github.com/karpathy/nanoGPT)
  - ä¸ºä»€ä¹ˆæ˜¯é¦–é€‰ï¼š
    - æç®€ã€å¹²å‡€ã€å¯ä¸€è¡Œä¸€è¡Œè¯»æ‡‚
    - è¦†ç›– LLM 90% çš„å…³é”®é€»è¾‘
    - å¯åœ¨ laptop / å• GPU è·‘é€š
  - ğŸ”¨ å»ºè®®ä½ åšçš„äº‹ï¼š
    - æ‰‹å†™ä¸€ç‰ˆä¸çœ‹ä»£ç 
    - å¯¹ç…§å®ç° causal mask
    - æ”¹å†™ attentionï¼ˆæ¯”å¦‚æ¢ RMSNormï¼‰
  - ğŸ“˜ é…å¥—æ–‡ç« ï¼š
    - Karpathy: Let's build GPT from scratch
  - ğŸ¯ é‡Œç¨‹ç¢‘ï¼š
    - èƒ½ç”¨ 50M å‚æ•°æ¨¡å‹ç”Ÿæˆåˆç†æ–‡æœ¬
- 2ï¸âƒ£ [minGPT â­â­â­â­â­ï¼ˆå¿…åšï¼‰](https://github.com/karpathy/minGPT)
  - ä¸ºä»€ä¹ˆæ˜¯é¦–é€‰ï¼š
    - è¿™æ˜¯ nanoGPT çš„å‰èº«ï¼Œæ›´æ•™å­¦å¯¼å‘ã€‚
  - ğŸ§ª ä½ åº”è¯¥èƒ½å›ç­”çš„é—®é¢˜
    - ä¸ºä»€ä¹ˆ attention è¦é™¤ä»¥ âˆšdï¼Ÿ
    - ä¸ºä»€ä¹ˆ LayerNorm åœ¨æ®‹å·®å‰/åï¼Ÿ
    - causal mask å¦‚ä½•å®ç°ï¼Ÿ

### é˜¶æ®µ 2ï¼šAttention å˜ä½“ & æ•°å€¼ç¨³å®šæ€§

ğŸ¯ ç›®æ ‡

ç†è§£ LLM çš„ç¨³å®šè®­ç»ƒä¸æ¨ç†ç»†èŠ‚


æ¨èæ–‡ç«  / repo
- ğŸ”¹ Attention ç¨³å®šæ€§
    - FlashAttention paper
    - RMSNorm paper
    - Pre-norm vs Post-norm
- ğŸ”¹ Linear Attention
    - Performer
    - Linformer
    - RetNetï¼ˆDeepMindï¼‰
- ğŸ“¦ æ¨è repoï¼š
    - https://github.com/HazyResearch/flash-attention
    - https://github.com/google-research/retention
- ğŸ¯ æ‰‹æ“ä»»åŠ¡ï¼š
    - æŠŠ softmax attention æ¢æˆçº¿æ€§ attention
    - æ¯”è¾ƒ loss / æ”¶æ•›é€Ÿåº¦

### é˜¶æ®µ 3ï¼šTokenizer & Embeddingï¼ˆå¾ˆå¤šäººå¿½ç•¥ï¼‰

ğŸ¯ ç›®æ ‡

ç†è§£ï¼š

- BPE / SentencePiece
- subword å½±å“
- vocab size tradeoff

ğŸ“¦ æ¨èï¼š

- https://github.com/google/sentencepiece
- HuggingFace tokenizers

ğŸ”¨ å®æˆ˜ï¼š

- è‡ªå·±è®­ç»ƒ tokenizer
- å¯¹æ¯” vocab=8k vs 32k


### é˜¶æ®µ 4ï¼šè®­ç»ƒæŠ€å·§ï¼ˆå·¥ä¸šçº§ï¼‰

ğŸ¯ ç›®æ ‡

ç†è§£ LLM èƒ½è·‘èµ·æ¥çš„å…³é”®æŠ€å·§

å¿…é¡»æŒæ¡

- Gradient clipping
- Learning rate warmup
- Weight decay
- Mixed precision
- Gradient accumulation

ğŸ“˜ æ¨èï¼š

- Scaling Laws for Neural Language Models
- HuggingFace training docs


### é˜¶æ®µ 5ï¼šæ¨ç† & è§£ç 

ğŸ¯ ç›®æ ‡

ç†è§£ï¼š

- Greedy / Top-k / Top-p
- Temperature
- KV cache
- é•¿æ–‡æœ¬ç”Ÿæˆ

ğŸ”¨ å®ç°ï¼š

- KV cache åŠ é€Ÿç”Ÿæˆ
- æ¯”è¾ƒæœ‰æ—  cache çš„é€Ÿåº¦

### é˜¶æ®µ 6ï¼šDeepMind é£æ ¼è¿›é˜¶é¡¹ç›®

#### é¡¹ç›® Aï¼šé•¿åºåˆ—è¯­è¨€å»ºæ¨¡

> â€œå¦‚ä½•è®© GPT çœ‹æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Ÿâ€

ä½ å¯ä»¥ï¼š

- å®ç° sliding window
- å®ç° RetNet
- å®ç° RoPE scaling

ğŸ¯ è¾“å‡ºï¼š

- å®éªŒå¯¹æ¯”
- GitHub repo + README

#### é¡¹ç›® Bï¼šLLM + åºåˆ—æ¨è

> æŠŠ LLM å½“ä½œç”¨æˆ·è¡Œä¸ºæ¨¡å‹

- è¾“å…¥ï¼šç”¨æˆ·äº‹ä»¶åºåˆ—ï¼ˆtokenizedï¼‰
- è¾“å‡ºï¼šæ˜¯å¦è½¬åŒ– / next event

ğŸ“¦ å‚è€ƒï¼š

- Transformer4Rec
- Decision Transformer

### DeepMind é¢è¯•çº§è¿½é—®ï¼ˆä½ å¿…é¡»èƒ½ç­”ï¼‰

> ä¸ºä»€ä¹ˆ causal LM å¯ä»¥ few-shotï¼Ÿ

> ä¸ºä»€ä¹ˆ LayerNorm ä¸ç”¨ BatchNormï¼Ÿ

> ä¸ºä»€ä¹ˆ KV cache æœ‰æ•ˆï¼Ÿ

> attention çš„ O(NÂ²) æ˜¯å¦æœ¬è´¨ï¼Ÿ

### å­¦ä¹ é¡ºåºï¼ˆæœ€æ¨èï¼‰

- 1ï¸âƒ£ nanoGPTï¼ˆ2â€“4 å‘¨ï¼‰
- 2ï¸âƒ£ Attention å˜ä½“ + æ•°å€¼ç¨³å®šæ€§ï¼ˆ2â€“3 å‘¨ï¼‰
- 3ï¸âƒ£ Tokenizer + Training tricksï¼ˆ2 å‘¨ï¼‰
- 4ï¸âƒ£ è‡ªé€‰ä¸€ä¸ªç ”ç©¶å‹é¡¹ç›®ï¼ˆ4â€“8 å‘¨ï¼‰


## References

- [How I got a job at DeepMind as a research engineer without a machine learning degree](https://gordicaleksa.medium.com/how-i-got-a-job-at-deepmind-as-a-research-engineer-without-a-machine-learning-degree-1a45f2a781de)
  - ML curriculum (read papers, implement, build projects)
    - Neural Style Transfer
    - DeepDream
    - Generative Adversarial Networks (GANs)
    - NLP & Transformers
    - Graph/Geometric ML
    - Reinforcement Learning
  - Write a blog at the end of each macro, summarize what you've learned
  - Open-source a project in the middle of the macro (implementation)
- [Deep Learning Journey Update: What Have I Learned About Transformers and NLP in 2 Months?](https://gordicaleksa.medium.com/deep-learning-journey-update-what-have-i-learned-about-transformers-and-nlp-in-2-months-eb6d31c0b848)